{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparkify Project Workspace\n",
    "This workspace contains a tiny subset (128MB) of the full dataset available (12GB). Feel free to use this workspace to build your project, or to explore a smaller subset with Spark before deploying your cluster on the cloud. Instructions for setting up your Spark cluster is included in the last lesson of the Extracurricular Spark Course content.\n",
    "\n",
    "You can follow the steps below to guide your data analysis and model building portion of this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.functions import desc\n",
    "from pyspark.sql.functions import asc\n",
    "from pyspark.sql.functions import sum as Fsum\n",
    "from pyspark.ml.feature import CountVectorizer, IDF, Normalizer, PCA, RegexTokenizer, StandardScaler, StopWordsRemover, StringIndexer, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.sql.functions import col, concat, count, lit, udf, avg\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Clean Dataset\n",
    "In this workspace, the mini-dataset file is `mini_sparkify_event_data.json`. Load and clean the dataset, checking for invalid or missing data - for example, records without userids or sessionids. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_filepath = \"mini_sparkify_event_data.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(database_filepath):\n",
    "    '''\n",
    "    The function creates a Spark session and loads the dataset from the json file into spark.\n",
    "    '''\n",
    "    # create a Spark session\n",
    "    spark = SparkSession.builder \\\n",
    "        .master(\"local\") \\\n",
    "        .appName(\"sparkify\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    user_log = spark.read.json(database_filepath) # database_filepath = \"mini_sparkify_event_data.json\"\n",
    "    return user_log\n",
    "    pass\n",
    "df = load_data('mini_sparkify_event_data.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(user_log):\n",
    "    '''\n",
    "    The function drops rows [\"userId\", \"sessionId\"] have Missing Values or empty from the inputed dataset\n",
    "                 add new cols ('Churn','chgrd','time_gap','sex') into dataset\n",
    "                 flag 'cancelled user' as 1, 'others' as 0 in col 'Churn'\n",
    "                 flag 'paid' usage in col 'chgrd'\n",
    "                 flag 'gender' in numeric format in col 'chgrd'\n",
    "                 get usage time by calculate max(ts) - min(ts) per user and saved the result in col 'time_gap'\n",
    "    '''\n",
    "    # drop rows [\"userId\", \"sessionId\"] have Missing Values or empty\n",
    "    user_log_valid = user_log.dropna(how = \"any\", subset = [\"userId\", \"sessionId\"])\n",
    "    user_log_valid = user_log_valid.filter(user_log_valid[\"userId\"] != \"\")\n",
    "\n",
    "    # find whether users Cancellation Confirmation and then flag'cancelled user' as 1, 'others' as 0 in 'Churn'.\n",
    "    flag_Cancel_event = udf(lambda x: 1 if x == \"Cancellation Confirmation\" else 0, IntegerType())\n",
    "    user_log_valid = user_log_valid.withColumn(\"Churn\", flag_Cancel_event(\"page\"))\n",
    "    windowval = Window.partitionBy(\"userId\").orderBy(desc(\"ts\")).rangeBetween(Window.unboundedPreceding, 0)\n",
    "    user_log_valid = user_log_valid.withColumn(\"Churn\", Fsum(\"Churn\").over(windowval))\n",
    "\n",
    "    # get numbers of page operations when the users' level = 'paid' and saved the numbers in column 'chgrd'.\n",
    "    flag_changegrade_event = udf(lambda x: 1 if x == 'paid' else 0, IntegerType())\n",
    "    user_log_valid = user_log_valid.withColumn(\"chgrd\", flag_changegrade_event(\"level\"))\n",
    "    windowval = Window.partitionBy(\"userId\").orderBy(desc(\"ts\")).rangeBetween(Window.unboundedPreceding, 0)\n",
    "    user_log_valid = user_log_valid.withColumn(\"chgrd\", Fsum(\"chgrd\").over(windowval))\n",
    "\n",
    "    # identify users' gender, flag male as 1 and female as 0 in 'sex'\n",
    "    flag_gender_event = udf(lambda x: 1 if x == \"M\" else 0, IntegerType())\n",
    "    user_log_valid = user_log_valid.withColumn(\"sex\", flag_gender_event(\"gender\"))\n",
    "\n",
    "    # create a temporary View and run SQL queries to get cleaned data df\n",
    "    user_log_valid.createOrReplaceTempView(\"user_log_valid_table\")\n",
    "    # cleaned data includes user Id, genda, paid usage(chrgd), users' stay time (time_gap), and cancel or not flag (Churn)\n",
    "    df = spark.sql(\n",
    "        '''\n",
    "        select t1.userId,sex,chgrd,time_gap,Churn\n",
    "        from user_log_valid_table t1\n",
    "        join\n",
    "         (select userId, max(chgrd) as mchgrd,(max(ts) - min(ts))/1000 as time_gap\n",
    "         from user_log_valid_table\n",
    "         group by userId) t2\n",
    "        on (t1.userId == t2.userId and t1.chgrd == t2.mchgrd)\n",
    "        order by Churn\n",
    "        '''\n",
    "                      ).dropDuplicates()\n",
    "    return df\n",
    "    pass\n",
    "\n",
    "df = clean_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Not used in the main()\n",
    "# Get YYYY-MM-DD-HH \n",
    "get_YYYY = udf(lambda x: datetime.datetime.fromtimestamp(x / 1000.0). year)\n",
    "get_MM = udf(lambda x: datetime.datetime.fromtimestamp(x / 1000.0). month)\n",
    "get_DD = udf(lambda x: datetime.datetime.fromtimestamp(x / 1000.0). day)\n",
    "get_HH = udf(lambda x: datetime.datetime.fromtimestamp(x / 1000.0). hour)\n",
    "\n",
    "#user_log = user_log.withColumn(\"Hour\", get_HH(user_log.ts))\n",
    "#user_log = user_log.withColumn('Year',get_YYYY(user_log.ts))\n",
    "#user_log = user_log.withColumn('Month',get_MM(user_log.ts))\n",
    "#user_log = user_log.withColumn('Day',get_DD(user_log.ts))\n",
    "#user_log.show()\n",
    "\n",
    "# add 'state' column\n",
    "splt = udf(lambda x:x.split(',')[1][1:3])\n",
    "user_log_valid = user_log_valid.withColumn('ST',splt('location'))\n",
    "#user_log_valid.select('ST').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis\n",
    "When you're working with the full dataset, perform EDA by loading a small subset of the data and doing basic manipulations within Spark. In this workspace, you are already provided a small subset of data you can explore.\n",
    "\n",
    "### Define Churn\n",
    "\n",
    "Once you've done some preliminary analysis, create a column `Churn` to use as the label for your model. I suggest using the `Cancellation Confirmation` events to define your churn, which happen for both paid and free users. As a bonus task, you can also look into the `Downgrade` events.\n",
    "\n",
    "### Explore Data\n",
    "Once you've defined churn, perform some exploratory data analysis to observe the behavior for users who stayed vs users who churned. You can start by exploring aggregates on these two groups of users, observing how much of a specific action they experienced per a certain time unit or number of songs played."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Insights:\n",
      " 1 Sex Disparity of Cancled User \n",
      "\n",
      "+-----+---+-----+\n",
      "|Churn|sex|count|\n",
      "+-----+---+-----+\n",
      "|    0|  1|   89|\n",
      "|    0|  0|   84|\n",
      "|    1|  1|   32|\n",
      "|    1|  0|   20|\n",
      "+-----+---+-----+\n",
      "\n",
      "\n",
      " 2 Usage Time Distribution \n",
      "\n",
      "+-----+-----------------+\n",
      "|Churn|    avg(time_gap)|\n",
      "+-----+-----------------+\n",
      "|    0|4060011.456647399|\n",
      "|    1|2031665.576923077|\n",
      "+-----+-----------------+\n",
      "\n",
      "\n",
      " 3 Usage paid usage Distribution \n",
      "\n",
      "+-----+------------------+\n",
      "|Churn|        avg(chgrd)|\n",
      "+-----+------------------+\n",
      "|    0|1098.0173410404625|\n",
      "|    1| 624.5384615384615|\n",
      "+-----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def explore_data(df):\n",
    "    '''\n",
    "    The function displays the Sex distribution,Usage Time distribution,\n",
    "    and paid usage Distribution between cancled users and noncancled users\n",
    "    '''\n",
    "    # display the sex distribution between cancled users and noncancled users\n",
    "    print('Insights:\\n 1 Sex Disparity of Cancled User \\n')\n",
    "    df.groupby(['Churn','sex']).count().show()\n",
    "\n",
    "    # display the usage time distribution between cancled users and noncancled users\n",
    "    print('\\n 2 Usage Time Distribution \\n')\n",
    "    df.groupby('Churn').avg('time_gap').show()\n",
    "\n",
    "    # display the paid usage (average of 'chgrd')distribution between cancled users and noncancled users\n",
    "    print('\\n 3 Usage paid usage Distribution \\n')\n",
    "    df.groupby('Churn').avg('chgrd').show()\n",
    "\n",
    "    pass\n",
    "explore_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not used\n",
    "#user_log_valid.select(\"page\").dropDuplicates().sort(\"page\").show() #\"auth\",'level','method','page','sessionId','status'\n",
    "#user_log_valid.select(\"userId\").where(user_log_valid.auth == 'Cancelled').collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "Once you've familiarized yourself with the data, build out the features you find promising to train your model on. To work with the full dataset, you can follow the following steps.\n",
    "- Write a script to extract the necessary features from the smaller subset of data\n",
    "- Ensure that your script is scalable, using the best practices discussed in Lesson 3\n",
    "- Try your script on the full data set, debugging your script if necessary\n",
    "\n",
    "If you are working in the classroom workspace, you can just extract features based on the small subset of data contained here. Be sure to transfer over this work to the larger dataset when you work on your Spark cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engi(df):\n",
    "    '''\n",
    "    The function Combine the gender, usage time, and paid usage columns into a vector,\n",
    "    and Scales the Vectors\n",
    "    '''\n",
    "    #Combine the gender, usage time, and paid usage columns into a vector\n",
    "    assembler = VectorAssembler(inputCols=[\"sex\",\"time_gap\",\"chgrd\"], outputCol=\"NumFeatures\")\n",
    "    df = assembler.transform(df)\n",
    "    #Scale the Vectors\n",
    "    scaler = StandardScaler(inputCol=\"NumFeatures\", outputCol=\"features\",withMean=True, withStd=False)\n",
    "    scalerModel = scaler.fit(df)\n",
    "    df = scalerModel.transform(df)\n",
    "    return df\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Not used\n",
    "#Normalize the Vectors\n",
    "scaler = Normalizer(inputCol=\"NumFeatures\", outputCol=\"ScaledNumFeatures\")\n",
    "df = scaler.transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "Split the full dataset into train, test, and validation sets. Test out several of the machine learning methods you learned. Evaluate the accuracy of the various models, tuning parameters as necessary. Determine your winning model based on test accuracy and report results on the validation set. Since the churned users are a fairly small subset, I suggest using F1 score as the metric to optimize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    '''\n",
    "    The function build model with\n",
    "    machine pipeline which take in the combined and scaled vector column \"ScaledNumFeatures\" as input and output\n",
    "    classification results on the 2 labels(\"Churn\") in the dataset.\n",
    "    '''\n",
    "    indexer = StringIndexer(inputCol=\"Churn\", outputCol=\"label\")\n",
    "    lr =  LogisticRegression(maxIter=10, regParam=0.0, elasticNetParam=0)\n",
    "    pipeline = Pipeline(stages=[indexer,lr])\n",
    "    #tune model\n",
    "    paramGrid = ParamGridBuilder() \\\n",
    "        .addGrid(lr.regParam,[0.0, 0.1]) \\\n",
    "        .addGrid(lr.maxIter,[10, 20]) \\\n",
    "        .build()\n",
    "    crossval = CrossValidator(estimator=pipeline,\n",
    "                              estimatorParamMaps=paramGrid,\n",
    "                              evaluator=MulticlassClassificationEvaluator(),\n",
    "                              numFolds=3)\n",
    "    return crossval\n",
    "    pass\n",
    "df = feature_engi(df)\n",
    "model = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "rest, validation = df.randomSplit([0.9, 0.1], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvmodel = model.fit(rest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Accuracy:\n",
      "\n",
      "0.5263157894736842\n",
      " Confusion Matrix:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.47      0.88      0.61         8\n",
      "        1.0       0.75      0.27      0.40        11\n",
      "\n",
      "avg / total       0.63      0.53      0.49        19\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, validation):\n",
    "    '''\n",
    "    The function reports the accuracy of the model prediction against validation dataset\n",
    "    '''\n",
    "    results = model.transform(validation)\n",
    "    # using classification_report to get F1 score\n",
    "    labels = np.unique(results.select ('prediction').collect())\n",
    "    confusion_mat = classification_report(results.select ('label').collect(), results.select ('prediction').collect(),labels=labels)\n",
    "    print(\" Accuracy:\\n\")\n",
    "    print((results.filter(results.label == results.prediction).count())/(results.count()))\n",
    "    print(\" Confusion Matrix:\\n \", confusion_mat)\n",
    "    return results\n",
    "    pass\n",
    "results = evaluate_model(cvmodel, validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def save_model(model, model_filepath):\n",
    "    '''\n",
    "    The function export tained model as a pickle file\n",
    "    '''\n",
    "    s = pickle.dumps(model.avgMetrics)\n",
    "    with open(model_filepath,'wb+') as f: # mode is'wb+'，represents binary writen\n",
    "        f.write(s)\n",
    "    pass\n",
    "save_model(cvmodel, 'cls.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#results_pd = results.toPandas()\n",
    "labels = np.unique(result.select ('prediction').toPandas()collect())\n",
    "confusion_mat = classification_report(result.select ('label').collect(), result.select ('prediction').collect(),labels=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Steps\n",
    "Clean up your code, adding comments and renaming variables to make the code easier to read and maintain. Refer to the Spark Project Overview page and Data Scientist Capstone Project Rubric to make sure you are including all components of the capstone project and meet all expectations. Remember, this includes thorough documentation in a README file in a Github repository, as well as a web app or blog post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "    DATABASE: -f\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "'Path does not exist: file:/home/workspace/-f;'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/opt/spark-2.4.3-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-2.4.3-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o354.json.\n: org.apache.spark.sql.AnalysisException: Path does not exist: file:/home/workspace/-f;\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary$1.apply(DataSource.scala:558)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary$1.apply(DataSource.scala:545)\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)\n\tat scala.collection.immutable.List.flatMap(List.scala:355)\n\tat org.apache.spark.sql.execution.datasources.DataSource.org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary(DataSource.scala:545)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:359)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:223)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n\tat org.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:391)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-e31be336e4eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-24-e31be336e4eb>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0mdatabase_filepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_filepath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loading data...\\n    DATABASE: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatabase_filepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m     \u001b[0muser_log\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatabase_filepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m     \u001b[0;31m# Train Test Split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-e31be336e4eb>\u001b[0m in \u001b[0;36mload_data\u001b[0;34m(database_filepath)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mspark\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m         \u001b[0;34m.\u001b[0m\u001b[0mmaster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"local\"\u001b[0m\u001b[0;34m)\u001b[0m         \u001b[0;34m.\u001b[0m\u001b[0mappName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sparkify\"\u001b[0m\u001b[0;34m)\u001b[0m         \u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0muser_log\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatabase_filepath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# database_filepath = \"mini_sparkify_event_data.json\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0muser_log\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-2.4.3-bin-hadoop2.7/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mjson\u001b[0;34m(self, path, schema, primitivesAsString, prefersDecimal, allowComments, allowUnquotedFieldNames, allowSingleQuotes, allowNumericLeadingZero, allowBackslashEscapingAnyCharacter, mode, columnNameOfCorruptRecord, dateFormat, timestampFormat, multiLine, allowUnquotedControlChars, lineSep, samplingRatio, dropFieldIfAllNull, encoding)\u001b[0m\n\u001b[1;32m    272\u001b[0m             \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-2.4.3-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-2.4.3-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: 'Path does not exist: file:/home/workspace/-f;'"
     ]
    }
   ],
   "source": [
    "#Sparkify Project Workspace\n",
    "# import libraries\n",
    "import sys\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.functions import desc\n",
    "from pyspark.sql.functions import asc\n",
    "from pyspark.sql.functions import sum as Fsum\n",
    "from pyspark.ml.feature import CountVectorizer, IDF, Normalizer, PCA, RegexTokenizer, StandardScaler, StopWordsRemover, StringIndexer, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.sql.functions import col, concat, count, lit, udf, avg\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# % matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def load_data(database_filepath):\n",
    "    '''\n",
    "    The function creates a Spark session and loads the dataset from the json file into spark.\n",
    "    '''\n",
    "    # create a Spark session\n",
    "    spark = SparkSession.builder \\\n",
    "        .master(\"local\") \\\n",
    "        .appName(\"sparkify\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    user_log = spark.read.json(database_filepath) # database_filepath = \"mini_sparkify_event_data.json\"\n",
    "    return user_log\n",
    "    pass\n",
    "\n",
    "def clean_data(user_log):\n",
    "    '''\n",
    "    The function drops rows [\"userId\", \"sessionId\"] have Missing Values or empty from the inputed dataset\n",
    "                 add new cols ('Churn','chgrd','time_gap','sex') into dataset\n",
    "                 flag 'cancelled user' as 1, 'others' as 0 in col 'Churn'\n",
    "                 flag 'paid' usage in col 'chgrd'\n",
    "                 flag 'gender' in numeric format in col 'chgrd'\n",
    "                 get usage time by calculate max(ts) - min(ts) per user and saved the result in col 'time_gap'\n",
    "    '''\n",
    "    # drop rows [\"userId\", \"sessionId\"] have Missing Values or empty\n",
    "    user_log_valid = user_log.dropna(how = \"any\", subset = [\"userId\", \"sessionId\"])\n",
    "    user_log_valid = user_log_valid.filter(user_log_valid[\"userId\"] != \"\")\n",
    "\n",
    "    # find whether users Cancellation Confirmation and then flag'cancelled user' as 1, 'others' as 0 in 'Churn'.\n",
    "    flag_Cancel_event = udf(lambda x: 1 if x == \"Cancellation Confirmation\" else 0, IntegerType())\n",
    "    user_log_valid = user_log_valid.withColumn(\"Churn\", flag_Cancel_event(\"page\"))\n",
    "    windowval = Window.partitionBy(\"userId\").orderBy(desc(\"ts\")).rangeBetween(Window.unboundedPreceding, 0)\n",
    "    user_log_valid = user_log_valid.withColumn(\"Churn\", Fsum(\"Churn\").over(windowval))\n",
    "\n",
    "    # get numbers of page operations when the users' level = 'paid' and saved the numbers in column 'chgrd'.\n",
    "    flag_changegrade_event = udf(lambda x: 1 if x == 'paid' else 0, IntegerType())\n",
    "    user_log_valid = user_log_valid.withColumn(\"chgrd\", flag_changegrade_event(\"level\"))\n",
    "    windowval = Window.partitionBy(\"userId\").orderBy(desc(\"ts\")).rangeBetween(Window.unboundedPreceding, 0)\n",
    "    user_log_valid = user_log_valid.withColumn(\"chgrd\", Fsum(\"chgrd\").over(windowval))\n",
    "\n",
    "    # identify users' gender, flag male as 1 and female as 0 in 'sex'\n",
    "    flag_gender_event = udf(lambda x: 1 if x == \"M\" else 0, IntegerType())\n",
    "    user_log_valid = user_log_valid.withColumn(\"sex\", flag_gender_event(\"gender\"))\n",
    "\n",
    "    # create a temporary View and run SQL queries to get cleaned data df\n",
    "    user_log_valid.createOrReplaceTempView(\"user_log_valid_table\")\n",
    "    # cleaned data includes user Id, genda, paid usage(chrgd), users' stay time (time_gap), and cancel or not flag (Churn)\n",
    "    df = spark.sql(\n",
    "        '''\n",
    "        select t1.userId,sex,chgrd,time_gap,Churn\n",
    "        from user_log_valid_table t1\n",
    "        join\n",
    "         (select userId, max(chgrd) as mchgrd,(max(ts) - min(ts))/1000 as time_gap\n",
    "         from user_log_valid_table\n",
    "         group by userId) t2\n",
    "        on (t1.userId == t2.userId and t1.chgrd == t2.mchgrd)\n",
    "        order by Churn\n",
    "        '''\n",
    "                      ).dropDuplicates()\n",
    "    return df\n",
    "    pass\n",
    "\n",
    "def explore_data(df):\n",
    "    '''\n",
    "    The function displays the Sex distribution,Usage Time distribution,\n",
    "    and paid usage Distribution between cancled users and noncancled users\n",
    "    '''\n",
    "    # display the sex distribution between cancled users and noncancled users\n",
    "    print('Insights:\\n 1 Sex Disparity of Cancled User \\n')\n",
    "    df.groupby(['Churn','sex']).count().show()\n",
    "\n",
    "    # display the usage time distribution between cancled users and noncancled users\n",
    "    print('\\n 2 Usage Time Distribution \\n')\n",
    "    df.groupby('Churn').avg('time_gap').show()\n",
    "\n",
    "    # display the paid usage (average of 'chgrd')distribution between cancled users and noncancled users\n",
    "    print('\\n 3 Usage paid usage Distribution \\n')\n",
    "    df.groupby('Churn').avg('chgrd').show()\n",
    "\n",
    "    pass\n",
    "\n",
    "def feature_engi(df):\n",
    "    '''\n",
    "    The function Combine the gender, usage time, and paid usage columns into a vector,\n",
    "    and Scales the Vectors\n",
    "    '''\n",
    "    #Combine the gender, usage time, and paid usage columns into a vector\n",
    "    assembler = VectorAssembler(inputCols=[\"sex\",\"time_gap\",\"chgrd\"], outputCol=\"NumFeatures\")\n",
    "    df = assembler.transform(df)\n",
    "    #Scale the Vectors\n",
    "    scaler = StandardScaler(inputCol=\"NumFeatures\", outputCol=\"features\",withMean=True, withStd=False)\n",
    "    scalerModel = scaler.fit(df)\n",
    "    df = scalerModel.transform(df)\n",
    "    return df\n",
    "    pass\n",
    "\n",
    "def build_model():\n",
    "    '''\n",
    "    The function build model with\n",
    "    machine pipeline which take in the combined and scaled vector column \"ScaledNumFeatures\" as input and output\n",
    "    classification results on the 2 labels(\"Churn\") in the dataset.\n",
    "    '''\n",
    "    indexer = StringIndexer(inputCol=\"Churn\", outputCol=\"label\")\n",
    "    lr =  LogisticRegression(maxIter=10, regParam=0.0, elasticNetParam=0)\n",
    "    pipeline = Pipeline(stages=[indexer,lr])\n",
    "    #tune model\n",
    "    paramGrid = ParamGridBuilder() \\\n",
    "        .addGrid(lr.regParam,[0.0, 0.1]) \\\n",
    "        .addGrid(lr.maxIter,[10, 20]) \\\n",
    "        .build()\n",
    "    crossval = CrossValidator(estimator=pipeline,\n",
    "                              estimatorParamMaps=paramGrid,\n",
    "                              evaluator=MulticlassClassificationEvaluator(),\n",
    "                              numFolds=3)\n",
    "    return crossval\n",
    "    pass\n",
    "\n",
    "def evaluate_model(model, validation):\n",
    "     '''\n",
    "    The function reports the accuracy of the model prediction against validation dataset\n",
    "    '''\n",
    "    results = model.transform(validation)\n",
    "    # using classification_report to get F1 score\n",
    "    labels = np.unique(results.select ('prediction').collect())\n",
    "    confusion_mat = classification_report(results.select ('label').collect(), results.select ('prediction').collect(),labels=labels)\n",
    "    print(\" Accuracy:\\n\")\n",
    "    print((results.filter(results.label == results.prediction).count())/(results.count()))\n",
    "    print(\" Confusion Matrix:\\n \", confusion_mat)\n",
    "    return results\n",
    "    pass\n",
    "\n",
    "def save_model(model, model_filepath):\n",
    "    '''\n",
    "    The function export tained model as a pickle file\n",
    "    '''\n",
    "    s = pickle.dumps(model)\n",
    "    with open(model_filepath,'wb+') as f: # mode is'wb+'，represents binary writen\n",
    "        f.write(s)\n",
    "    pass\n",
    "\n",
    "def main():\n",
    "    '''\n",
    "    The main() function combines and executes all the above modules.\n",
    "    '''\n",
    "    database_filepath, model_filepath = sys.argv[1:]\n",
    "    print('Loading data...\\n    DATABASE: {}'.format(database_filepath))\n",
    "    user_log = load_data(database_filepath)\n",
    "\n",
    "    # Train Test Split\n",
    "    #break data set into 90% of training data and set aside 10%. Set random seed to 42.\n",
    "    rest, validation = df.randomSplit([0.9, 0.1], seed=42)\n",
    "\n",
    "    print('Building model...')\n",
    "    model = build_model()\n",
    "\n",
    "    #Train pipeline\n",
    "    print('Training model...')\n",
    "    model.fit(rest)\n",
    "\n",
    "    print('Evaluating model...')\n",
    "    evaluate_model(validation)\n",
    "\n",
    "    print('Saving model...\\n    MODEL: {}'.format(model_filepath))\n",
    "    save_model(model, model_filepath)\n",
    "\n",
    "    print('Trained model saved!')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
